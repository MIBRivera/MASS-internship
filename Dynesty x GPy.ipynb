{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20708c23",
   "metadata": {},
   "source": [
    "From ChatGPT:\n",
    "\n",
    "Using Nested Sampling to marginalize over the likelihood while selecting hyperparameters for the kernel of a Gaussian process is a valuable approach in Bayesian model selection and hyperparameter optimization. This technique allows you to consider the uncertainty in the hyperparameters and efficiently explore the model space. Here's a step-by-step guide on how to do this:\n",
    "\n",
    "1. **Set Up the Gaussian Process Model**:\n",
    "   - Define your Gaussian process model, including the kernel and the likelihood function. The kernel typically includes hyperparameters that you want to optimize or marginalize over.\n",
    "\n",
    "2. **Choose Priors for Hyperparameters**:\n",
    "   - Define prior distributions for the hyperparameters of the kernel. Priors represent your prior beliefs about the values of these hyperparameters. The choice of priors depends on your prior knowledge about the system or problem.\n",
    "\n",
    "3. **Construct the Likelihood Function**:\n",
    "   - Build the likelihood function that quantifies how well the model with specific hyperparameters fits the observed data. This function should take the model's predictions and compare them to the observed data while incorporating the likelihood of measurement errors.\n",
    "\n",
    "4. **Configure Nested Sampling**:\n",
    "   - Choose a Nested Sampling algorithm (e.g., MultiNest, Dynesty) and configure it. Set parameters such as the number of live points, stopping criteria, and the likelihood function defined in the previous step. The hyperparameters of the kernel should also be treated as parameters in the Nested Sampling run.\n",
    "\n",
    "5. **Run Nested Sampling**:\n",
    "   - Execute the Nested Sampling algorithm with the configured settings. Nested Sampling will explore the parameter space, including the hyperparameters of the kernel, and provide samples from the posterior distribution of these hyperparameters.\n",
    "\n",
    "6. **Estimate the Evidence**:\n",
    "   - One of the primary goals of Nested Sampling is to estimate the Bayesian evidence, which quantifies how well the model with specific hyperparameters fits the data. The evidence accounts for the uncertainty in the hyperparameters and the model's goodness of fit.\n",
    "\n",
    "7. **Obtain Posterior Samples**:\n",
    "   - In addition to estimating the evidence, Nested Sampling provides posterior samples of the hyperparameters. These samples represent the most likely values for the hyperparameters given the data and allow you to quantify the uncertainty in hyperparameter estimates.\n",
    "\n",
    "8. **Analyze and Select Hyperparameters**:\n",
    "   - Analyze the posterior samples of the hyperparameters to determine the most likely values and their uncertainties. You can use this information for model selection, hyperparameter optimization, and to make more informed decisions about the choice of the kernel's hyperparameters.\n",
    "\n",
    "9. **Model Comparison and Selection**:\n",
    "   - You can perform model comparison by comparing the Bayesian evidence for different kernel hyperparameters. The model with the highest evidence is the most favored by the data.\n",
    "\n",
    "10. **Repeat if Necessary**:\n",
    "    - If your prior beliefs about the hyperparameters change or you need to update the model, you can repeat the Nested Sampling process to explore the new parameter space and obtain updated results.\n",
    "\n",
    "By using Nested Sampling in this way, you can effectively marginalize over the likelihood and estimate the posterior distribution of kernel hyperparameters, accounting for the uncertainties in the model and data. This approach is particularly useful in scenarios where hyperparameter selection is essential for the performance of the Gaussian process model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8509a282",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tester' from 'numpy.testing' (C:\\Users\\Marco Immanuel\\anaconda3\\lib\\site-packages\\numpy\\testing\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2056\\3971521265.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mGPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdynesty\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\GPy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Tester' from 'numpy.testing' (C:\\Users\\Marco Immanuel\\anaconda3\\lib\\site-packages\\numpy\\testing\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GPy\n",
    "import dynesty as dy\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281266bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some synthetic data\n",
    "x = np.linspace(0.,2*np.pi,100)[:,None]\n",
    "y = -np.cos(x)+np.random.randn(*x.shape)*0.3+1\n",
    "_ = plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b012f4",
   "metadata": {},
   "source": [
    "Using a RBF kernel,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749240f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a GP regression model\n",
    "m = GPy.models.GPRegression(x,y)\n",
    "# Give some general prior distributions for model parameters\n",
    "m.kern.lengthscale.set_prior(GPy.priors.Gamma.from_EV(1.,10.))\n",
    "m.kern.variance.set_prior(GPy.priors.Gamma.from_EV(1.,10.))\n",
    "m.likelihood.variance.set_prior(GPy.priors.Gamma.from_EV(1.,10.))\n",
    "_=m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775f0af",
   "metadata": {},
   "source": [
    "# Dynesty part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee68453",
   "metadata": {},
   "source": [
    "Again, dynesty requires three inputs: number of dimensions, log likelihood, and prior transform. Problems encountered:\n",
    "\n",
    "1. What is the likelihood we need to implement? A: **The marginalized likelihood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c33ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensionality of our problem.\n",
    "ndim = 3\n",
    "\n",
    "# Define our 3-D correlated multivariate normal likelihood.\n",
    "C = np.identity(ndim)  # set covariance to identity matrix\n",
    "C[C==0] = 0.95  # set off-diagonal terms\n",
    "Cinv = np.linalg.inv(C)  # define the inverse (i.e. the precision matrix)\n",
    "lnorm = -0.5 * (np.log(2 * np.pi) * ndim +\n",
    "                np.log(np.linalg.det(C)))  # ln(normalization)\n",
    "\n",
    "def loglike(x):\n",
    "    \"\"\"The log-likelihood function.\"\"\"\n",
    "\n",
    "    return -0.5 * np.dot(x, np.dot(Cinv, x)) + lnorm\n",
    "\n",
    "# Define our uniform prior.\n",
    "def ptform(u):\n",
    "    \"\"\"Transforms the uniform random variables `u ~ Unif[0., 1.)`\n",
    "    to the parameters of interest.\"\"\"\n",
    "    \n",
    "    x = np.array(u) # Copy u\n",
    "    \n",
    "    alpha = 10.\n",
    "    for i in range(2):\n",
    "    # Gamma\n",
    "        x[i] = scipy.stats.gamma.ppf(u[i], alpha)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea96601",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dy.NestedSampler(loglike, ptform, ndim)\n",
    "sampler.run_nested()\n",
    "results = sampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51bb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynesty import plotting as dyplot\n",
    "_, _ = dyplot.cornerplot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynesty import utils as dyfunc\n",
    "\n",
    "# Extract sampling results.\n",
    "samples = results.samples  # samples\n",
    "weights = results.importance_weights()\n",
    "\n",
    "# Compute weighted mean and covariance.\n",
    "mean, cov = dyfunc.mean_and_cov(samples, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb99085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model parameters as the posterior mean\n",
    "m.kern.variance[:] = samples[:,0].mean()\n",
    "m.kern.lengthscale[:] = samples[:,1].mean()\n",
    "m.likelihood.variance[:] = samples[:,2].mean()\n",
    "print(m)\n",
    "_=m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db437a76",
   "metadata": {},
   "source": [
    "## Verdict: ???/10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
